{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example of using ChatGPT API for article summarization\n",
        "Refer to: https://colab.research.google.com/drive/15jh4v_TBPsTyIBhi0Fz46gEkjvhzGaBR?usp=sharing\n"
      ],
      "metadata": {
        "id": "DdTgmpZxHvrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "l7ZgKhclHnbJ",
        "outputId": "1a03a37d-d0ed-4799-db23-ac2b49bde2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-122231703.py:59: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_ui = gr.Chatbot(height=420, show_copy_button=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed3a997c621a15e333.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed3a997c621a15e333.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# If needed in Colab:\n",
        "# !pip -q install openai==1.43.0 gradio==4.44.0\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- API key: use Colab Secrets (preferred) or env var ---\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "except Exception:\n",
        "    pass\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in Colab Secrets or env.\")\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"   # you can switch to \"gpt-4o\"\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful, concise assistant. \"\n",
        "    \"Always use the full conversation context to avoid repeating information.\"\n",
        ")\n",
        "\n",
        "def chat(user_input, chatbot, temp, max_toks):\n",
        "    # Build message list from prior turns so the model has context\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for input_text, response_text in chatbot:\n",
        "        if input_text:\n",
        "            messages.append({\"role\": \"user\", \"content\": input_text})\n",
        "        if response_text:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "    # Add the new user turn\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Call Chat Completions API with full history\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        temperature=temp,\n",
        "        max_tokens=max_toks,\n",
        "    )\n",
        "\n",
        "    assistant_reply = response.choices[0].message.content\n",
        "\n",
        "    # Update the visible chat history so Gradio shows all turns\n",
        "    chatbot = chatbot + [[user_input, assistant_reply]]\n",
        "    # Return \"\" to clear the textbox, and updated chatbot history\n",
        "    return \"\", chatbot\n",
        "\n",
        "# ----------------- Gradio UI -----------------\n",
        "with gr.Blocks(title=\"Context-Aware ChatGPT\") as demo:\n",
        "    gr.Markdown(\"### Chatbot with Conversation History\\n\"\n",
        "                \"All previous turns are sent to the API so the model keeps context.\")\n",
        "    chatbot_ui = gr.Chatbot(height=420, show_copy_button=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(placeholder=\"Type your messageâ€¦\", label=\"Your message\", scale=4)\n",
        "        send = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    with gr.Row():\n",
        "        temp = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label=\"Temperature\")\n",
        "        max_toks = gr.Slider(64, 512, value=200, step=16, label=\"Max tokens\")\n",
        "\n",
        "    clear = gr.Button(\"Clear history\")\n",
        "\n",
        "    # Wire up events: both Enter and Send use the same chat() function\n",
        "    msg.submit(chat, inputs=[msg, chatbot_ui, temp, max_toks], outputs=[msg, chatbot_ui])\n",
        "    send.click(chat, inputs=[msg, chatbot_ui, temp, max_toks], outputs=[msg, chatbot_ui])\n",
        "    clear.click(lambda: None, None, chatbot_ui, queue=False)\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ]
}